# Standard Library Imports
import os
import sys
import time
import logging
import json
import math
import asyncio # Added asyncio
from datetime import datetime, timedelta
from collections import defaultdict
import traceback # Often useful, ensure it's there if used
import pytz # If timezone operations are direct in this file
import copy

# Third-party Imports
import numpy as np
import pandas as pd
import torch
import torch.nn as nn # For direct nn module usage like MSELoss, HuberLoss etc.
from torch.utils.data import TensorDataset, DataLoader
import torch.optim as optim # Can use this or from torch.optim import Adam, etc.
from torch.optim.lr_scheduler import ReduceLROnPlateau, OneCycleLR # Add OneCycleLR if implementing
from torch.nn.utils import clip_grad_norm_
import yfinance as yf # If direct yf calls are made here, else only in data_fetcher
import joblib
import requests
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error
from sklearn.model_selection import train_test_split # Added train_test_split
from scipy.stats import ks_2samp # If check_data_drift uses it directly here
from cachetools import TTLCache
import psutil
import ta # Technical Analysis library
import torch.nn.functional as F

# FastAPI Imports (if this controller is directly used by FastAPI)
from fastapi import HTTPException 

# Project-specific Imports
import simple_config  # Simple config for API keys
from controllers.data_fetcher import DataFetcher
from controllers.model_manager import ModelManager
from controllers.error_handler import ErrorTracker, ErrorSeverity, ErrorCategory
from controllers.data_validator import DataValidator, rate_limit # Check if rate_limit is used here or just in DataValidator
from controllers.data_quality import DataQualityMetrics
from ml_models.bilstm_predictor import BiLSTMWithAttention
from controllers.early_stopping import EarlyStopping  # Add this import
from controllers.feature_engineering import calculate_technical_indicators

# Type Hinting
from typing import Tuple, Dict, Any, Optional, List # List was missing in my previous example

# Specific torch.nn components if you prefer explicit imports over nn.Module
# from torch.nn import Linear, ReLU, Sigmoid, Tanh, Dropout, BatchNorm1d, LSTM, GRU, RNN # etc. 
# The generic `import torch.nn as nn` is usually sufficient if you use `nn.Linear`, `nn.LSTM`.

# Utility Functions
def convert_numpy_types(obj):
    """Convert numpy types to native Python types for JSON serialization"""
    if isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, np.bool_):
        return bool(obj)
    else:
        return obj



def get_next_timestamp(last_timestamp, timeframe):
    """Calculate next timestamp based on timeframe with timezone handling"""
    timeframe_deltas = {
        "30m": pd.Timedelta(minutes=30),
        "1h": pd.Timedelta(hours=1),
        "4h": pd.Timedelta(hours=4),
        "24h": pd.Timedelta(days=1)
    }
    now = datetime.now(pytz.UTC)
    if last_timestamp.tzinfo is None:
        last_timestamp = last_timestamp.tz_localize('UTC')
    if timeframe == "24h":
        next_ts = (last_timestamp + pd.Timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)
    else:
        next_ts = last_timestamp + timeframe_deltas[timeframe]
    if next_ts <= now:
        periods = int((now - last_timestamp) / timeframe_deltas[timeframe]) + 1
        if timeframe == "24h":
            next_ts = (last_timestamp + periods * timeframe_deltas[timeframe]).replace(hour=0, minute=0, second=0, microsecond=0)
        else:
            next_ts = last_timestamp + periods * timeframe_deltas[timeframe]
    return next_ts

# Allowlist datetime.datetime for safe model loading
torch.serialization.add_safe_globals([datetime])

# Setup logging
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')

# Constants from simple_config - Optimized for Accuracy
MODEL_PATH = simple_config.MODEL_PATH
MAX_RETRIES = 3
RETRY_DELAY = 2
EPOCHS = 300  # Increased from 200 for better accuracy
PATIENCE = 40  # Increased patience for better convergence 
CACHE_DIR = "cache"
BATCH_SIZE = simple_config.BATCH_SIZE
MIN_MEMORY_GB = 0.75
VALIDATION_SPLIT = simple_config.VALIDATION_SPLIT

# Cache for sentiment and price data
SENTIMENT_CACHE = TTLCache(maxsize=100, ttl=simple_config.CACHE_TTL)
PRICE_CACHE = TTLCache(maxsize=10, ttl=60)

# CoinGecko ID mapping
# Global cache for CoinGecko IDs to avoid repeated API calls
coin_id_cache = {}

async def get_coingecko_id(symbol: str) -> Optional[str]:
    """Dynamically get CoinGecko ID from symbol using predefined mapping for major cryptocurrencies"""
    try:
        symbol = symbol.upper()
        
        # Predefined mapping for major cryptocurrencies to avoid confusion
        major_crypto_mapping = {
            'BTC': 'bitcoin',
            'ETH': 'ethereum',
            'USDT': 'tether',
            'BNB': 'binancecoin',
            'SOL': 'solana',
            'XRP': 'ripple',
            'USDC': 'usd-coin',
            'STETH': 'staked-ether',
            'ADA': 'cardano',
            'AVAX': 'avalanche-2',
            'DOGE': 'dogecoin',
            'TRX': 'tron',
            'TON': 'the-open-network',
            'SHIB': 'shiba-inu',
            'DOT': 'polkadot',
            'WBTC': 'wrapped-bitcoin',
            'BCH': 'bitcoin-cash',
            'LINK': 'chainlink',
            'LTC': 'litecoin',
            'MATIC': 'matic-network',
            'UNI': 'uniswap',
            'LEO': 'leo-token',
            'XLM': 'stellar',
            'ETC': 'ethereum-classic',
            'OKB': 'okb',
            'ICP': 'internet-computer',
            'ATOM': 'cosmos',
            'FIL': 'filecoin',
            'HBAR': 'hedera-hashgraph',
            'CRO': 'crypto-com-chain',
            'ARB': 'arbitrum',
            'VET': 'vechain',
            'MNT': 'mantle',
            'DAI': 'dai',
            'IMX': 'immutable-x',
            'APT': 'aptos',
            'OP': 'optimism',
            'NEAR': 'near',
            'TUSD': 'true-usd',
            'INJ': 'injective-protocol',
            'FIRST': 'first-digital-usd',
            'GRT': 'the-graph',
            'LDO': 'lido-dao',
            'MKR': 'maker',
            'AAVE': 'aave',
            'SNX': 'synthetix-network-token',
            'QNT': 'quant-network',
            'RUNE': 'thorchain',
            'FLOW': 'flow',
            'SAND': 'the-sandbox',
            'MANA': 'decentraland',
            'APE': 'apecoin',
            'ALGO': 'algorand',
            'EGLD': 'elrond-erd-2',
            'XTZ': 'tezos',
            'CHZ': 'chiliz',
            'EOS': 'eos',
            'AXS': 'axie-infinity',
            'THETA': 'theta-token',
            'FTM': 'fantom',
            'ICP': 'internet-computer',
            'XMR': 'monero',
            'KLAY': 'klay-token',
            'BSV': 'bitcoin-sv',
            'NEO': 'neo',
            'USDD': 'usdd',
            'FRAX': 'frax',
            'CAKE': 'pancakeswap-token',
            'CRV': 'curve-dao-token',
            'ZEC': 'zcash',
            'DASH': 'dash'
        }
        
        # Check predefined mapping first
        if symbol in major_crypto_mapping:
            coin_id = major_crypto_mapping[symbol]
            logger.info(f"Found CoinGecko ID for {symbol} from predefined mapping: {coin_id}")
            return coin_id
        
        # Check cache for non-major cryptocurrencies
        if symbol in coin_id_cache:
            logger.debug(f"Found cached CoinGecko ID for {symbol}: {coin_id_cache[symbol]}")
            return coin_id_cache[symbol]
        
        # For non-major cryptocurrencies, use API call
        url = "https://api.coingecko.com/api/v3/coins/list"
        response = requests.get(url, timeout=10)
        
        if response.status_code != 200:
            logger.warning(f"Failed to fetch CoinGecko coins list: {response.status_code}")
            return None
            
        coins_data = response.json()
        
        # Search for the symbol in the coins list (only for non-major coins)
        for coin in coins_data:
            if coin.get('symbol', '').upper() == symbol:
                coin_id = coin.get('id')
                coin_id_cache[symbol] = coin_id  # Cache the result
                logger.info(f"Found CoinGecko ID for {symbol}: {coin_id}")
                return coin_id
        
        logger.warning(f"No CoinGecko ID found for symbol: {symbol}")
        return None
        
    except Exception as e:
        logger.error(f"Error getting CoinGecko ID for {symbol}: {e}")
        return None

# Add after imports
model_manager = ModelManager()
error_tracker = ErrorTracker()

# New synchronous helper function for training data - CHANGING TO ASYNC
async def _get_and_prepare_training_data(symbol: str, timeframe: str, lookback: int, test_size: float = simple_config.VALIDATION_SPLIT) -> tuple:
    """Get and prepare training data with proper tensor shapes"""
    try:
        logger.info(f"Starting training data preparation for {symbol} ({timeframe}), lookback={lookback}")
        
        # Get historical data and prepare features
        features_df, target_series = await prepare_features(symbol, timeframe)
        
        # Get the definitive list of features for model input
        feature_column_names_for_model_input = list(features_df.columns)
        
        logger.info(f"Using definitive feature columns for model input: {feature_column_names_for_model_input}")

        # Ensure all columns designated for model input are actually in features_df
        missing_cols_in_df = [col for col in feature_column_names_for_model_input if col not in features_df.columns]
        if missing_cols_in_df:
            raise ValueError(f"Missing columns in features_df: {missing_cols_in_df}")
        
        # Extract features and target
        X_data_to_sequence = features_df[feature_column_names_for_model_input].values
        y_data = target_series.values
        
        # Log data shapes before scaling
        logger.info(f"[_GET_AND_PREPARE_TRAINING_DATA] Shape of data PASSED to scaler.fit() (X_data_to_sequence): {X_data_to_sequence.shape}")
        logger.info(f"[_GET_AND_PREPARE_TRAINING_DATA] Columns of data PASSED to scaler.fit() (X_data_to_sequence): {feature_column_names_for_model_input}")
        
        # Scale features
        feature_scaler = StandardScaler()
        X_data_scaled = feature_scaler.fit_transform(X_data_to_sequence)
        
        # Scale target
        target_scaler = StandardScaler()
        y_data_scaled = target_scaler.fit_transform(y_data.reshape(-1, 1))
        
        # Log target scaling info
        logger.info(f"[_GET_AND_PREPARE_TRAINING_DATA] Target scaling - center: {target_scaler.mean_[0]}, scale: {target_scaler.scale_[0]}")
        logger.info(f"[_GET_AND_PREPARE_TRAINING_DATA] Target range before scaling - min: {y_data.min()}, max: {y_data.max()}")
        logger.info(f"[_GET_AND_PREPARE_TRAINING_DATA] Target range after scaling - min: {y_data_scaled.min()}, max: {y_data_scaled.max()}")
        
        # Create sequences
        X_sequences = []
        y_sequences = []
        
        for i in range(len(X_data_scaled) - lookback):
            X_sequences.append(X_data_scaled[i:(i + lookback)])
            y_sequences.append(y_data_scaled[i + lookback])
        
        # Convert to numpy arrays with correct shapes
        X_sequences = np.array(X_sequences)  # Shape: (n_samples, lookback, n_features)
        y_sequences = np.array(y_sequences)  # Shape: (n_samples, 1)
        
        # Split into training and validation sets
        split_idx = int(len(X_sequences) * (1 - test_size))
        
        X_train = X_sequences[:split_idx]
        y_train = y_sequences[:split_idx]
        X_val = X_sequences[split_idx:]
        y_val = y_sequences[split_idx:]
        
        # Convert to PyTorch tensors with correct shapes
        X_train = torch.FloatTensor(X_train)  # Shape: (n_train, lookback, n_features)
        y_train = torch.FloatTensor(y_train).view(-1, 1)  # Shape: (n_train, 1)
        X_val = torch.FloatTensor(X_val)  # Shape: (n_val, lookback, n_features)
        y_val = torch.FloatTensor(y_val).view(-1, 1)  # Shape: (n_val, 1)
        
        # Log final shapes
        logger.info(f"Training data: X_train {X_train.shape}, y_train {y_train.shape}")
        logger.info(f"Validation data: X_val {X_val.shape}, y_val {y_val.shape}")
        
        return X_train, y_train, X_val, y_val, feature_scaler, target_scaler, feature_column_names_for_model_input, features_df
        
    except Exception as e:
        logger.error(f"Error in _get_and_prepare_training_data: {str(e)}")
        raise

@rate_limit(api_name='yfinance')
def fetch_yfinance_data(symbol, period, interval, timeframe: str = None): # ADDED timeframe ARG
    """Cached yfinance data fetch with improved error handling and data validation"""
    try:
        # This part still assumes direct yfinance call or that DataFetcher aligns
        ticker_obj = yf.Ticker(f"{symbol.upper()}-USD")
        data = ticker_obj.history(period=period, interval=interval, auto_adjust=True)

        if data is None or data.empty:
            error_id = error_tracker.track_error(
                ValueError(f"Could not fetch yfinance data for {symbol}"),
                ErrorSeverity.ERROR,
                ErrorCategory.DATA_FETCH,
                "yfinance_direct_fetcher", # Changed source
                 metadata={"symbol": symbol, "period": period, "interval": interval}
            )
            # raise ValueError(f"Data fetch failed (Error ID: {error_id})") # Allow fallback
            logger.error(f"YFinance direct fetch for {symbol} returned no data (Error ID: {error_id}).")
            return pd.DataFrame()
            
        # Standardize column names
        data.rename(columns={"Open": "open", "High": "high", "Low": "low", "Close": "close", "Volume": "volume"}, inplace=True, errors='ignore')
        data.columns = data.columns.str.capitalize()

        # Determine the correct timeframe key for DataValidator
        validator_timeframe_key = timeframe # Use the passed 'timeframe' (e.g., "24h")
        if not validator_timeframe_key or validator_timeframe_key not in simple_config.TIMEFRAME_MAP:
            # Fallback or mapping if the main 'timeframe' isn't directly usable
            # This logic might need to be more robust if 'interval' can be varied independently
            logger.warning(f"Validator timeframe key '{validator_timeframe_key if validator_timeframe_key else 'None'}' is invalid or not in TIMEFRAME_MAP. Attempting to use 'interval' ('{interval}') or a default.")
            # Simple fallback: if interval is a key, use it. Otherwise, try to map or use default.
            if interval in simple_config.TIMEFRAME_MAP:
                validator_timeframe_key = interval
            else: # Last resort, try to find a match or use default. This could be improved.
                compatible_key = next((k for k, v in simple_config.TIMEFRAME_MAP.items() if v.get("interval") == interval), None)
                if compatible_key:
                    validator_timeframe_key = compatible_key
                else:
                    validator_timeframe_key = simple_config.DEFAULT_TIMEFRAME
                    logger.error(f"Could not map yfinance interval '{interval}' to a valid TIMEFRAME_MAP key. Using default: '{validator_timeframe_key}'")

        logger.debug(f"DataValidator will use timeframe_key: '{validator_timeframe_key}' (derived from input timeframe: '{timeframe}', interval: '{interval}')")
        validator = DataValidator(timeframe=validator_timeframe_key)
        data, metrics = validator.validate_and_clean_data(data)
        
        if metrics.overall_quality < 0.7: 
            error_id = error_tracker.track_error(
                ValueError(f"Data quality below threshold. Metrics: {metrics.to_dict()}"),
                ErrorSeverity.WARNING,
                ErrorCategory.DATA_QUALITY,
                "data_validator_yfinance",
                metadata={"quality_metrics": metrics.to_dict()} 
            )
            logger.warning(f"Low quality data (Error ID: {error_id}). Metrics: {metrics.to_dict()}")
            
        return data
        
    except Exception as e:
        error_id = error_tracker.track_error(
            e, ErrorSeverity.ERROR,
            ErrorCategory.DATA_FETCH,
            "fetch_yfinance_data_general_error",
            metadata={"symbol": symbol, "period": period, "interval": interval, "error_details": str(e)}
        )
        logger.error(f"Error fetching yfinance data (Error ID: {error_id}): {str(e)}")
        # raise # Or return empty pd.DataFrame() to allow fallback
        return pd.DataFrame()

def check_server(timeout: int = 2) -> bool:
    """Check if FastAPI server is running"""
    try:
        response = requests.get("http://localhost:8000/api/health", timeout=timeout)
        return response.status_code == 200
    except requests.exceptions.RequestException:
        logger.error("FastAPI server is not running")
        return False

@rate_limit(api_name='coingecko')
async def fetch_coingecko_price(symbol: str):
    """Fetch real-time price from CoinGecko with improved error handling"""
    cache_key = f"price_{symbol}"
    if cache_key in PRICE_CACHE:
        logger.debug(f"Returning cached price for {symbol}")
        return PRICE_CACHE[cache_key]

    coin_id = await get_coingecko_id(symbol)
    if not coin_id:
        error_id = error_tracker.track_error(
            ValueError(f"No CoinGecko ID mapping for {symbol}"),
            ErrorSeverity.ERROR,
            ErrorCategory.DATA_FETCH,
            "coingecko_fetcher"
        )
        logger.error(f"Invalid symbol (Error ID: {error_id})")
        return None

    url = f"{simple_config.COINGECKO_API_URL}/simple/price"
    headers = {
        'accept': 'application/json'
    }
    api_key = simple_config.COINGECKO_API_KEY
    if api_key:
        headers['x-cg-demo-api-key'] = api_key
        
    params = {
        'ids': coin_id,
        'vs_currencies': 'usd',
        'include_24hr_change': 'true'
    }

    try:
        response = requests.get(url, headers=headers, params=params, timeout=15)
        
        if response.status_code == 200:
            data = response.json()
            if coin_id in data and 'usd' in data[coin_id]:
                price = float(data[coin_id]['usd'])
                PRICE_CACHE[cache_key] = price
                logger.debug(f"Fetched price from API for {symbol}: {price}")
                return price
        else:
            error_message = f"API request failed with status {response.status_code}. Response: {response.text}"
            error_id = error_tracker.track_error(
                Exception(error_message),
                ErrorSeverity.ERROR,
                ErrorCategory.API,
                "coingecko_api"
            )
            logger.error(f"API request failed (Error ID: {error_id})")
            
    except requests.exceptions.RequestException as e:
        error_id = error_tracker.track_error(
            e, ErrorSeverity.ERROR,
            ErrorCategory.API,
            "coingecko_api"
        )
        logger.error(f"API request error (Error ID: {error_id}): {str(e)}")

    return None

async def fetch_coingecko_sentiment(symbol: str):
    """Fetch sentiment from CoinGecko with improved error handling"""
    cache_key = f"sentiment_{symbol}"
    if cache_key in SENTIMENT_CACHE:
        logger.debug(f"Returning cached sentiment for {symbol}")
        return SENTIMENT_CACHE[cache_key]

    coin_id = await get_coingecko_id(symbol)
    if not coin_id:
        logger.warning(f"No CoinGecko mapping for symbol {symbol}, using default sentiment")
        return 0.5

    url = f"{simple_config.COINGECKO_API_URL}/coins/{coin_id}"
    headers = {
        'accept': 'application/json'
    }
    api_key = simple_config.COINGECKO_API_KEY
    if api_key:
        headers['x-cg-demo-api-key'] = api_key

    try:
        response = requests.get(url, headers=headers, timeout=15)
        
        if response.status_code == 200:
            data = response.json()
            sentiment = data.get('sentiment_votes_up_percentage', 50) / 100
            logger.debug(f"Fetched sentiment from API for {symbol}: {sentiment}")
            SENTIMENT_CACHE[cache_key] = sentiment
            return sentiment
        else:
            error_message = f"API request failed with status {response.status_code}. Response: {response.text}"
            logger.error(f"API request failed: {error_message}") # Simplified logging for now, can re-add ErrorTracker if needed
            
    except requests.exceptions.RequestException as e:
        logger.error(f"API request error: {str(e)}") # Simplified logging for now

    return 0.5  # Default neutral sentiment

@rate_limit(api_name='coingecko') 
async def fetch_coingecko_fallback(coin_id, timeframe):
    """Fallback to CoinGecko historical data with improved rate limiting"""
    try:
        # Use period from TIMEFRAME_MAP
        days_str = simple_config.TIMEFRAME_MAP[timeframe].get('period', '30d')
        days = int(days_str[:-1]) # Convert '30d' to 30
        
        # Use the dynamic lookup instead of hardcoded mapping
        mapped_coin_id = await get_coingecko_id(coin_id)
        if not mapped_coin_id:
            mapped_coin_id = coin_id.lower()  # fallback to lowercase
        
        url = f"{simple_config.COINGECKO_API_URL}/coins/{mapped_coin_id}/market_chart"
        headers = {
            'accept': 'application/json'
        }
        api_key = simple_config.COINGECKO_API_KEY
        if api_key:
            headers['x-cg-demo-api-key'] = api_key
            
        params = {
            'vs_currency': 'usd',
            'days': str(days),
            'interval': 'hourly' if days <= 90 else 'daily'
        }
        
        backoff = 1  # Initial backoff time in seconds
        for attempt in range(MAX_RETRIES):
            try:
                resp = requests.get(url, headers=headers, params=params, timeout=15)
                
                if resp.status_code == 200:
                    data = resp.json()
                    if not data or 'prices' not in data:
                        logger.error(f"Invalid response format from CoinGecko for {coin_id}")
                        return pd.DataFrame()
                        
                    # Convert data to DataFrame
                    prices = data['prices']
                    volumes = data.get('total_volumes', [[0, 0]] * len(prices))
                    
                    df = pd.DataFrame(prices, columns=['timestamp', 'price'])
                    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms', utc=True)
                    df.set_index('timestamp', inplace=True)
                    
                    # Add volume data
                    volume_df = pd.DataFrame(volumes, columns=['timestamp', 'volume'])
                    volume_df['timestamp'] = pd.to_datetime(volume_df['timestamp'], unit='ms', utc=True)
                    volume_df.set_index('timestamp', inplace=True)
                    
                    # Create OHLCV DataFrame
                    final_df = pd.DataFrame(index=df.index)
                    final_df['Close'] = df['price']
                    final_df['Volume'] = volume_df['volume']
                    final_df['Open'] = final_df['Close'].shift(1)
                    final_df['High'] = final_df['Close'].rolling(2, min_periods=1).max()
                    final_df['Low'] = final_df['Close'].rolling(2, min_periods=1).min()
                    
                    # Forward fill any missing data
                    final_df = final_df.ffill().bfill()
                    
                    logger.debug(f"CoinGecko fallback fetched {len(final_df)} rows for {coin_id}")
                    return final_df
                    
                elif resp.status_code == 429:  # Rate limit hit
                    wait_time = backoff * (2 ** attempt)
                    logger.warning(f"Rate limit hit, waiting {wait_time}s before retry")
                    time.sleep(wait_time)
                    continue
                    
                else:
                    logger.error(f"CoinGecko fallback failed with status code {resp.status_code}")
                    if attempt < MAX_RETRIES - 1:
                        time.sleep(RETRY_DELAY)
                        continue
                    return pd.DataFrame()
                    
            except requests.exceptions.RequestException as e:
                logger.error(f"Request error on attempt {attempt + 1}: {str(e)}")
                if attempt < MAX_RETRIES - 1:
                    time.sleep(RETRY_DELAY)
                    continue
                    
        logger.warning(f"CoinGecko fallback failed for {coin_id} after {MAX_RETRIES} attempts")
        return pd.DataFrame()
        
    except Exception as e:
        logger.error(f"CoinGecko fallback failed: {str(e)}")
        return pd.DataFrame()

async def get_latest_data(symbol: str, timeframe: str = "24h") -> pd.DataFrame: # ASYNC
    try:
        data_fetcher = DataFetcher()
        data = await data_fetcher.get_merged_data(symbol, timeframe) # AWAITED
        
        if data is None or data.empty:
            logger.error(f"Data fetch for {symbol} timeframe {timeframe} returned None or empty after get_merged_data.")
            raise ValueError(f"Could not fetch sufficient data for {symbol} (timeframe: {timeframe}) after get_merged_data call.")

        # Ensure TIMEFRAME_MAP from simple_config is used to get lookback
        if timeframe not in simple_config.TIMEFRAME_MAP or "lookback" not in simple_config.TIMEFRAME_MAP[timeframe]:
            logger.error(f"Timeframe '{timeframe}' or its lookback configuration not found in simple_config.TIMEFRAME_MAP.")
            raise ValueError(f"Invalid timeframe configuration for {timeframe}.")
            
        required_lookback = simple_config.TIMEFRAME_MAP[timeframe]["lookback"]
        
        if len(data) < required_lookback:
            logger.error(f"Insufficient data for {symbol} ({timeframe}): fetched {len(data)} points, but model requires lookback of {required_lookback} points.")
            raise ValueError(f"Insufficient data points for {symbol} ({timeframe}) to meet model lookback requirement ({len(data)} < {required_lookback}).")
            
        logger.info(f"Successfully fetched sufficient data for {symbol}, timeframe {timeframe} in get_latest_data. Shape: {data.shape}, Lookback satisfied: {len(data)} >= {required_lookback}")
        return data
        
    except ValueError as ve:
        logger.error(f"ValueError in get_latest_data for {symbol} ({timeframe}): {str(ve)}", exc_info=True) 
        raise
    except Exception as e:
        logger.error(f"Unexpected error in get_latest_data for {symbol} ({timeframe}): {type(e).__name__} - {str(e)}", exc_info=True)
        raise ValueError(f"Unexpected error fetching latest data for {symbol} ({timeframe}) due to {type(e).__name__}.") from e

def ensemble_prediction(symbol, timeframes=["1h", "4h", "24h"]):
    """Combine predictions from multiple timeframes with dynamic weighting"""
    predictions = {}
    mape_scores = {}
    for tf in timeframes:
        result = predict_next_price(symbol, tf)
        if "error" not in result:
            predictions[tf] = result["predicted_price"]
            mape_scores[tf] = result.get("mape", 100)
    
    if not predictions:
        logger.error("No valid predictions for ensemble")
        return predict_next_price(symbol, "24h")
    
    total_inverse_mape = sum(1 / max(mape, 1e-8) for mape in mape_scores.values())
    weights = {tf: (1 / max(mape_scores[tf], 1e-8)) / total_inverse_mape for tf in predictions}
    weighted_avg = sum(predictions[tf] * weights[tf] for tf in predictions)
    
    last_result = predict_next_price(symbol, "24h")
    last_result["predicted_price"] = weighted_avg
    last_result["ensemble"] = predictions
    last_result["weights"] = weights
    return last_result

def fetch_market_data(symbol: str, timeframe: str):
    """Fetch market data with improved error handling and fallback mechanisms"""
    try:
        logger.info(f"Fetching market data for {symbol} with timeframe {timeframe}")
        
        tf_config = simple_config.TIMEFRAME_MAP.get(timeframe)
        if not tf_config:
            raise ValueError(f"Invalid timeframe '{timeframe}' for TIMEFRAME_MAP in fetch_market_data")
        
        yfinance_period = tf_config.get("period")
        yfinance_interval = tf_config.get("interval") # yfinance specific, e.g. "1d"

        if not yfinance_period or not yfinance_interval:
            raise ValueError(f"Missing period or interval for timeframe '{timeframe}' in TIMEFRAME_MAP")

        logger.debug(f"Attempting to fetch data from yfinance for {symbol} using TIMEFRAME_MAP key '{timeframe}' (period: {yfinance_period}, interval: {yfinance_interval})")
        # Pass the main 'timeframe' (e.g. "24h") to fetch_yfinance_data
        primary_data = fetch_yfinance_data(symbol, period=yfinance_period, interval=yfinance_interval, timeframe=timeframe)
        
        # Validate primary data
        if primary_data is not None and not primary_data.empty:
            primary_data.timeframe = timeframe  # Add timeframe attribute for validation
            if validate_data_quality(primary_data):
                logger.info(f"Successfully fetched and validated {len(primary_data)} points from primary source")
                return primary_data
            else:
                logger.warning("Primary data failed validation")
        
        # Try CoinGecko as fallback
        logger.debug(f"Attempting to fetch data from CoinGecko for {symbol}")
        coingecko_data = fetch_coingecko_fallback(symbol, timeframe)
        
        if coingecko_data is not None and not coingecko_data.empty:
            coingecko_data.timeframe = timeframe  # Add timeframe attribute for validation
            if validate_data_quality(coingecko_data):
                logger.info(f"Successfully fetched and validated {len(coingecko_data)} points from CoinGecko")
                return coingecko_data
            else:
                logger.warning("CoinGecko data failed validation")
        
        # If both sources failed, try to merge them
        if primary_data is not None and coingecko_data is not None:
            logger.info("Attempting to merge data from both sources")
            merged_data = merge_data_sources(primary_data, coingecko_data)
            merged_data.timeframe = timeframe
            
            if validate_data_quality(merged_data):
                logger.info(f"Successfully merged and validated {len(merged_data)} points")
                return merged_data
        
        raise ValueError(f"Could not fetch sufficient valid data for {symbol} with timeframe {timeframe}")
        
    except Exception as e:
        logger.error(f"Error fetching market data: {str(e)}")
        raise

def merge_data_sources(primary_data: pd.DataFrame, secondary_data: pd.DataFrame) -> pd.DataFrame:
    """Merge data from multiple sources with careful handling of overlaps and conflicts"""
    try:
        # Ensure both dataframes have datetime index
        for df in [primary_data, secondary_data]:
            if not isinstance(df.index, pd.DatetimeIndex):
                df.index = pd.to_datetime(df.index)
            if df.index.tz is None:
                df.index = df.index.tz_localize('UTC')
        
        # Combine the data
        combined = pd.concat([primary_data, secondary_data])
        combined = combined[~combined.index.duplicated(keep='first')]
        combined.sort_index(inplace=True)
        
        # Handle gaps
        max_gap = pd.Timedelta(hours=24)
        gaps = combined.index[1:] - combined.index[:-1]
        large_gaps = gaps > max_gap
        
        if large_gaps.any():
            logger.warning(f"Found {large_gaps.sum()} large gaps in merged data")
            
        # Interpolate missing values
        combined = combined.interpolate(method='time', limit=3)
        
        # Validate final dataset
        if len(combined) < len(primary_data) and len(combined) < len(secondary_data):
            logger.warning("Merged dataset is smaller than both inputs")
            
        return combined
        
    except Exception as e:
        logger.error(f"Error merging data sources: {str(e)}")
        raise

def detect_support_resistance(data: pd.DataFrame, feature_scaler=None, feature_names=None, window: int = 20, num_points: int = 5):
    """Detect support and resistance levels using price action and volume"""
    try:
        if len(data) < window * 2 + 1:
            logger.warning(f"Insufficient data for support/resistance detection: {len(data)} < {window * 2 + 1}")
            return {'resistance': [], 'support': []}
            
        highs = data['High'].values
        lows = data['Low'].values
        volumes = data['Volume'].values
        
        # DEBUG: Log the actual price ranges to verify if data is normalized
        logger.info(f"[SUPPORT_RESISTANCE_DEBUG] Input data shape: {data.shape}")
        logger.info(f"[SUPPORT_RESISTANCE_DEBUG] High range: {highs.min():.6f} to {highs.max():.6f}")
        logger.info(f"[SUPPORT_RESISTANCE_DEBUG] Low range: {lows.min():.6f} to {lows.max():.6f}")
        logger.info(f"[SUPPORT_RESISTANCE_DEBUG] Close range: {data['Close'].min():.6f} to {data['Close'].max():.6f}")
        logger.info(f"[SUPPORT_RESISTANCE_DEBUG] Sample Close values (last 5): {data['Close'].tail().values}")
        
        # Check if data is normalized and we need to denormalize
        high_is_normalized = highs.max() < 100 and highs.min() > -10  # Normalized data is typically in range [-3, 3]
        close_is_real = data['Close'].min() > 1000  # Real crypto prices are typically > $1000 for major coins
        
        logger.info(f"[SUPPORT_RESISTANCE_DEBUG] High appears normalized: {high_is_normalized}")
        logger.info(f"[SUPPORT_RESISTANCE_DEBUG] Close appears real price: {close_is_real}")
        
        # Find local maxima and minima
        resistance_points = []
        support_points = []
        
        logger.info(f"[SUPPORT_RESISTANCE] Analyzing {len(data)} data points with window={window}")
        
        for i in range(window, len(data) - window):
            # High points for resistance - check if current high is the highest in the window
            is_resistance = True
            for j in range(i - window, i + window + 1):
                if j != i and highs[j] > highs[i]:
                    is_resistance = False
                    break
            
            if is_resistance:
                resistance_points.append((data.index[i], highs[i], volumes[i]))
                
            # Low points for support - check if current low is the lowest in the window
            is_support = True
            for j in range(i - window, i + window + 1):
                if j != i and lows[j] < lows[i]:
                    is_support = False
                    break
                    
            if is_support:
                support_points.append((data.index[i], lows[i], volumes[i]))
        
        logger.info(f"[SUPPORT_RESISTANCE] Found {len(resistance_points)} resistance and {len(support_points)} support points")
        
        # Weight points by volume and recency
        def weight_points(points):
            if not points:
                return []
            weights = []
            max_volume = max(p[2] for p in points)
            latest_time = max(p[0] for p in points)
            
            for point in points:
                volume_weight = point[2] / max_volume
                # Fix time calculation - use days instead of seconds for better scaling
                time_diff_days = (latest_time - point[0]).total_seconds() / 86400
                time_weight = max(0.1, 1 - time_diff_days / 30)  # 30 days max, minimum 0.1
                total_weight = volume_weight * 0.7 + time_weight * 0.3  # Weight volume more
                weights.append((point[1], total_weight))
            
            # Group close prices (within 2% range)
            grouped = []
            for price, weight in sorted(weights, key=lambda x: x[0]):
                added = False
                for i, (group_price, group_weight) in enumerate(grouped):
                    if abs(price - group_price) / max(group_price, 1e-8) < 0.02:
                        grouped[i] = ((group_price * group_weight + price * weight) / (group_weight + weight),
                                    group_weight + weight)
                        added = True
                        break
                if not added:
                    grouped.append((price, weight))
            
            # Sort by weight and take top points
            return sorted([(price, weight) for price, weight in grouped], 
                        key=lambda x: x[1], reverse=True)[:num_points]
    
        resistance_levels = weight_points(resistance_points)
        support_levels = weight_points(support_points)
        
        logger.info(f"[SUPPORT_RESISTANCE] Final levels - Resistance: {len(resistance_levels)}, Support: {len(support_levels)}")
        
        # Denormalize support/resistance levels if they appear to be normalized
        def denormalize_levels(levels, level_type):
            if not levels:
                return levels
                
            denormalized_levels = []
            for price, weight in levels:
                original_price = price
                
                # Check if this looks like normalized data (typically in range [-3, 3])
                if -10 < price < 10 and close_is_real:
                    try:
                        # Use Close price statistics to estimate the real price scale
                        close_values = data['Close'].values
                        close_mean = np.mean(close_values)
                        close_std = np.std(close_values)
                        
                        # For resistance levels, use High column scaling estimate
                        # For support levels, use Low column scaling estimate
                        if level_type == 'resistance':
                            # Estimate High prices from Close prices (High is typically Close + some percentage)
                            high_estimate = close_mean * (1 + 0.01)  # Assume ~1% above close on average
                            denormalized_price = high_estimate + (price * close_std * 0.1)  # Scale normalized value
                        else:
                            # Estimate Low prices from Close prices (Low is typically Close - some percentage)
                            low_estimate = close_mean * (1 - 0.01)  # Assume ~1% below close on average  
                            denormalized_price = low_estimate + (price * close_std * 0.1)  # Scale normalized value
                        
                        logger.info(f"[SUPPORT_RESISTANCE_DENORM_V2] {level_type} level: {original_price:.6f} -> {denormalized_price:.2f}")
                        price = denormalized_price
                    except Exception as e:
                        logger.warning(f"[SUPPORT_RESISTANCE_DENORM_V2] Failed to denormalize {level_type} level: {e}")
                        # Keep original price if denormalization fails
                
                # Only include levels with positive prices that make sense for crypto
                if price > 1000:  # Reasonable minimum for major crypto prices
                    denormalized_levels.append((float(price), float(weight)))
                else:
                    logger.warning(f"[SUPPORT_RESISTANCE_DENORM_V2] Filtered out unrealistic {level_type} price: {price}")
            
            return denormalized_levels
        
        # Apply denormalization
        resistance_levels = denormalize_levels(resistance_levels, 'resistance')
        support_levels = denormalize_levels(support_levels, 'support')
        
        logger.info(f"[SUPPORT_RESISTANCE] After denormalization - Resistance: {resistance_levels}")
        logger.info(f"[SUPPORT_RESISTANCE] After denormalization - Support: {support_levels}")
        
        return {
            'resistance': resistance_levels,
            'support': support_levels
        }
        
    except Exception as e:
        logger.error(f"Error detecting support/resistance: {str(e)}", exc_info=True)
        return {'resistance': [], 'support': []}

def calculate_prediction_probability(data: pd.DataFrame, prediction: float, model_confidence: float, 
                                  support_resistance: dict, timeframe: str, raw_technical_indicators: dict = None):
    """Calculate probability metrics for the prediction"""
    try:
        current_price = data['Close'].iloc[-1]
        
        # Base probability from model confidence
        base_prob = float(model_confidence)  # Ensure float conversion
        
        # Analyze support/resistance levels with bounds checking
        price_probs = []
        for level_type in ['support', 'resistance']:
            for price, weight in support_resistance[level_type]:
                # Ensure values are float and positive
                price = float(price)
                weight = float(weight)
                
                # Skip invalid prices (negative or zero)
                if price <= 0:
                    continue
                    
                # Ensure weight is positive
                weight = abs(weight)
                
                # Calculate proximity to S/R level
                proximity = abs(prediction - price) / max(price, 1e-8)  # Prevent division by zero
                if proximity < 0.02:  # Within 2% of S/R level
                    signal_type = "bounce" if level_type == "support" and prediction > price else \
                                  "reversal" if level_type == "resistance" and prediction < price else \
                                  "breakout"
                    
                    # Ensure confidence is positive and bounded
                    confidence = max(0, weight * (1 - proximity/0.02))
                    
                    price_probs.append({
                        'type': level_type,
                        'price': price,
                        'strength': weight,
                        'proximity': float(proximity),
                        'signal': signal_type,
                        'confidence': float(confidence)
                    })
        
        # Technical indicator probabilities
        tech_probs = []
        
        # RSI Analysis - Use raw values if available
        if raw_technical_indicators and 'RSI_14' in raw_technical_indicators:
            rsi_value = raw_technical_indicators['RSI_14']
            logger.info(f"[PREDICT_PROB] Using raw RSI value: {rsi_value}")
        else:
            rsi_value = None
            rsi_cols = ['RSI_14', 'RSI']
            for col in rsi_cols:
                if col in data.columns:
                    rsi_value = data[col].iloc[-1]
                    # Check if RSI is scaled and convert back to 0-100 range
                    if 0 <= rsi_value <= 1:
                        rsi_value = rsi_value * 100
                        logger.info(f"[PREDICT_PROB] Converted scaled RSI {data[col].iloc[-1]} to {rsi_value}")
                    else:
                        logger.info(f"[PREDICT_PROB] Using {col} column for RSI analysis. Value: {rsi_value}")
                    break
        
        if rsi_value is not None and 0 <= rsi_value <= 100:
            rsi_signal = {
                'indicator': 'RSI',
                'value': float(rsi_value),
                'signal': 'overbought' if rsi_value > 70 else 'oversold' if rsi_value < 30 else 'neutral',
                'strength': 0.7 if (rsi_value > 70 and prediction < current_price) or 
                                    (rsi_value < 30 and prediction > current_price) else 0.3
            }
            tech_probs.append(rsi_signal)
        
        # MACD Analysis - Use raw values if available
        if raw_technical_indicators and 'MACD' in raw_technical_indicators:
            macd = raw_technical_indicators['MACD']
            macd_signal = raw_technical_indicators.get('MACD_Signal', 0)
            logger.info(f"[PREDICT_PROB] Using raw MACD values: {macd}, signal: {macd_signal}")
            
            trend_strength = abs(macd - macd_signal)
            macd_signal_obj = {
                'indicator': 'MACD',
                'value': float(macd),
                'signal_line': float(macd_signal),
                'signal': 'bullish_trend' if macd > macd_signal else 'bearish_trend',
                'strength': 0.4 + min(0.3, trend_strength / 10),
                'trend_strength': float(trend_strength)
            }
            tech_probs.append(macd_signal_obj)
        elif all(col in data.columns for col in ['MACD', 'MACD_Signal']):
            macd = data['MACD'].iloc[-1]
            macd_signal = data['MACD_Signal'].iloc[-1]
            macd_prev = data['MACD'].iloc[-2] if len(data) > 1 else macd
            macd_signal_prev = data['MACD_Signal'].iloc[-2] if len(data) > 1 else macd_signal
            
            # Detect crossovers and trend strength
            crossover = (macd > macd_signal and macd_prev < macd_signal_prev) or \
                       (macd < macd_signal and macd_prev > macd_signal_prev)
            trend_strength = abs(macd - macd_signal) / abs(macd_signal) if abs(macd_signal) > 0 else 0
            
            macd_signal_obj = {
                'indicator': 'MACD',
                'value': float(macd),
                'signal_line': float(macd_signal),
                'signal': 'bullish_crossover' if macd > macd_signal and macd_prev < macd_signal_prev else
                         'bearish_crossover' if macd < macd_signal and macd_prev > macd_signal_prev else
                         'bullish_trend' if macd > macd_signal else 'bearish_trend',
                'strength': 0.65 if crossover else 0.4 + min(0.3, trend_strength),
                'trend_strength': float(trend_strength)
            }
            tech_probs.append(macd_signal_obj)
        
        # Bollinger Bands Analysis - Use raw values if available
        bb_upper = None
        bb_lower = None
        bb_middle = None
        
        if raw_technical_indicators and all(key in raw_technical_indicators for key in ['Bollinger_Upper', 'Bollinger_Lower', 'Bollinger_middle']):
            bb_upper = raw_technical_indicators['Bollinger_Upper']
            bb_lower = raw_technical_indicators['Bollinger_Lower'] 
            bb_middle = raw_technical_indicators['Bollinger_middle']
            logger.info(f"[PREDICT_PROB] Using raw Bollinger bands: Upper={bb_upper}, Lower={bb_lower}, Middle={bb_middle}")
        else:
            bb_cols = {
                'upper': ['Bollinger_Upper', 'BB_upper'],
                'lower': ['Bollinger_Lower', 'BB_lower'],
                'middle': ['Bollinger_middle', 'BB_middle']
            }
            
            bb_values = {}
            for key, possible_cols in bb_cols.items():
                for col in possible_cols:
                    if col in data.columns:
                        bb_values[key] = data[col].iloc[-1]
                        break
            
            bb_upper = bb_values.get('upper', current_price)
            bb_lower = bb_values.get('lower', current_price)
            bb_middle = bb_values.get('middle', current_price)
        
        if bb_upper is not None and bb_lower is not None and bb_middle is not None and bb_middle > 0:
            bb_position = 'above_upper' if current_price > bb_upper else \
                         'below_lower' if current_price < bb_lower else \
                         'within_bands'
            
            # Calculate bandwidth correctly (always positive)
            bandwidth = abs(bb_upper - bb_lower) / bb_middle
            
            bb_signal = {
                'indicator': 'BollingerBands',
                'position': bb_position,
                'signal': 'reversal' if (bb_position == 'above_upper' and prediction < current_price) or
                                      (bb_position == 'below_lower' and prediction > current_price) else
                         'continuation',
                'strength': 0.6 if bb_position != 'within_bands' else 0.4,
                'bandwidth': float(bandwidth)
            }
            tech_probs.append(bb_signal)
        
        # Volume Analysis
        if 'Volume' in data.columns and len(data) >= 20:
            vol_sma = data['Volume'].rolling(20).mean().iloc[-1]
            current_vol = data['Volume'].iloc[-1]
            vol_ratio = current_vol / vol_sma if vol_sma > 0 else 1.0
            
            volume_signal = {
                'indicator': 'Volume',
                'value': float(current_vol),
                'sma': float(vol_sma),
                'ratio': float(vol_ratio),
                'signal': 'high' if vol_ratio > 1.5 else 'low' if vol_ratio < 0.5 else 'normal',
                'strength': min(0.8, 0.4 + 0.2 * vol_ratio) if vol_ratio > 1 else \
                           max(0.2, 0.4 - 0.2 * (1 - vol_ratio))
            }
            tech_probs.append(volume_signal)
        
        # Combine probabilities with weighted approach
        combined_prob = base_prob
        
        # Weight the technical probabilities
        if tech_probs:
            tech_weight = min(0.6, 0.2 * len(tech_probs))  # Cap technical weight at 0.6
            tech_prob = sum(p['strength'] for p in tech_probs) / len(tech_probs)
            combined_prob = (combined_prob * (1 - tech_weight)) + (tech_prob * tech_weight)
        
        # Add support/resistance influence
        if price_probs:
            sr_weight = min(0.4, 0.15 * len(price_probs))  # Cap S/R weight at 0.4
            sr_prob = sum(p['confidence'] for p in price_probs) / len(price_probs)
            combined_prob = (combined_prob * (1 - sr_weight)) + (sr_prob * sr_weight)
        
        # Adjust for timeframe reliability
        timeframe_factors = {
            "1m": 0.7,
            "5m": 0.75,
            "15m": 0.8,
            "30m": 0.85,
            "1h": 0.9,
            "4h": 0.95,
            "24h": 1.0
        }
        timeframe_factor = timeframe_factors.get(timeframe, 0.85)
        combined_prob *= timeframe_factor
        
        # Final probability capped between 0.1 and 0.95
        final_prob = min(max(combined_prob, 0.1), 0.95)
        
        # Enhanced confidence factors with more detail
        confidence_factors = {
            'model_confidence': base_prob,
            'price_levels': price_probs,
            'technical_indicators': tech_probs,
            'timeframe_factor': timeframe_factor,
            'available_indicators': {
                'rsi': rsi_value is not None,
                'macd': 'MACD' in data.columns and 'MACD_Signal' in data.columns,
                'bollinger': bb_upper is not None and bb_lower is not None and bb_middle is not None,
                'volume': 'Volume' in data.columns
            },
            'weights': {
                'model': 1 - (tech_weight if tech_probs else 0) - (sr_weight if price_probs else 0),
                'technical': tech_weight if tech_probs else 0,
                'support_resistance': sr_weight if price_probs else 0
            }
        }
        
        return {
            'probability': final_prob,
            'confidence_factors': confidence_factors
        }
        
    except Exception as e:
        logger.error(f"Error calculating prediction probability: {str(e)}")
        # Return a more informative default response
        return {
            'probability': 0.5,  # Neutral probability
            'confidence_factors': {
                'model_confidence': model_confidence,
                'error': str(e),
                'available_columns': list(data.columns) if isinstance(data, pd.DataFrame) else 'No DataFrame'
            }
        }

async def predict_next_price(symbol, timeframe="24h", force_retrain=False):
    """Predict the next price for a given symbol and timeframe - uses existing models by default"""
    try:
        logger.info(f"Starting prediction for {symbol} {timeframe} - force_retrain={force_retrain}")
        
        # Get model (will load existing or train new if needed)
        model, feature_scaler, target_scaler, feature_names, training_metrics = await get_model(symbol, timeframe, force_retrain=force_retrain)
        
        # Get hyperparameters for this symbol/timeframe  
        timeframe_config = simple_config.TIMEFRAME_MAP.get(timeframe, {})
        lookback = timeframe_config.get('lookback', 72)
        
        # Model is already in eval mode from get_model
        model.eval()
        
        # Get latest data
        latest_data = await get_latest_data(symbol, timeframe)
        
        if latest_data is None or len(latest_data) < lookback:
            raise ValueError(f"Insufficient data points. Need at least {lookback} points.")
        
        # Store raw data for technical analysis before any scaling
        raw_data = latest_data.copy()
        
        # Calculate technical indicators on raw data and store before any scaling
        raw_indicators_df = calculate_technical_indicators(raw_data.copy())
        raw_technical_indicators = {
            'RSI_14': float(raw_indicators_df['RSI_14'].iloc[-1]) if 'RSI_14' in raw_indicators_df.columns else 50.0,
            'MACD': float(raw_indicators_df['MACD'].iloc[-1]) if 'MACD' in raw_indicators_df.columns else 0.0,
            'MACD_Signal': float(raw_indicators_df['MACD_Signal'].iloc[-1]) if 'MACD_Signal' in raw_indicators_df.columns else 0.0,
            'Bollinger_Upper': float(raw_indicators_df['Bollinger_Upper'].iloc[-1]) if 'Bollinger_Upper' in raw_indicators_df.columns else 0.0,
            'Bollinger_Lower': float(raw_indicators_df['Bollinger_Lower'].iloc[-1]) if 'Bollinger_Lower' in raw_indicators_df.columns else 0.0,
            'Bollinger_middle': float(raw_indicators_df['Bollinger_middle'].iloc[-1]) if 'Bollinger_middle' in raw_indicators_df.columns else 0.0,
        }
        logger.info(f"[PREDICT_NEXT_PRICE] Raw technical indicators: {raw_technical_indicators}")
        
        # Prepare features using the same feature names from training
        features_df, _ = await prepare_features(symbol, timeframe)
        features = features_df[feature_names].values
        
        # Scale features
        scaled_features = feature_scaler.transform(features)
        
        # Create sequence for prediction (take last lookback points)
        X = scaled_features[-lookback:].reshape(1, lookback, -1)  # Let reshape infer the last dimension
        
        # Convert to tensor and make prediction
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)
        
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X).to(device)
            prediction = model(X_tensor)
            prediction = prediction.cpu().numpy()
        
        # Inverse transform prediction
        prediction_denorm = target_scaler.inverse_transform(prediction.reshape(-1, 1))
        predicted_value = float(prediction_denorm[-1, 0])
        
        # Get last actual price for comparison
        last_price = float(latest_data['Close'].iloc[-1])
        
        # Convert prediction based on target type
        if timeframe in ['1m', '5m', '15m', '30m', '1h']:
            # For short timeframes, prediction is percentage change
            predicted_price = last_price * (1 + predicted_value / 100)
            logger.info(f"[PREDICT_NEXT_PRICE] Converted percentage change {predicted_value:.4f}% to price: {last_price:.2f} -> {predicted_price:.2f}")
        else:
            # For longer timeframes, prediction is absolute price
            predicted_price = predicted_value
            logger.info(f"[PREDICT_NEXT_PRICE] Using absolute price prediction: {predicted_price:.2f}")
        
        # Get truly raw data (before any scaling) for support/resistance detection
        try:
            data_fetcher = DataFetcher()
            # Get raw data without any technical indicators or scaling
            raw_unscaled_data = await data_fetcher.fetch_data(symbol, timeframe)
            if raw_unscaled_data is None or len(raw_unscaled_data) < 20:
                logger.warning("Could not fetch raw unscaled data, using processed data")
                raw_unscaled_data = raw_data
        except Exception as e:
            logger.warning(f"Error fetching raw unscaled data: {e}, using processed data")
            raw_unscaled_data = raw_data
        
        # Calculate confidence metrics using truly raw data for support/resistance
        support_resistance = detect_support_resistance(raw_unscaled_data, feature_scaler, feature_names)
        probability = calculate_prediction_probability(
            raw_data,  # Use raw data for price analysis
            predicted_price,
            0.8,  # Model confidence
            support_resistance,
            timeframe,
            raw_technical_indicators  # Pass raw technical indicators
        )
        
        # Get next timestamp
        next_timestamp = get_next_timestamp(latest_data.index[-1], timeframe)
        
        # Use the metrics returned from get_model
        final_metrics = training_metrics if training_metrics else {}
        
        # Format support/resistance levels for API response
        price_levels = []
        
        # Add resistance levels
        for price, weight in support_resistance.get('resistance', []):
            price_levels.append({
                'type': 'resistance',
                'price': float(price),
                'strength': float(weight)
            })
        
        # Add support levels  
        for price, weight in support_resistance.get('support', []):
            price_levels.append({
                'type': 'support', 
                'price': float(price),
                'strength': float(weight)
            })
        
        logger.info(f"[PREDICT_NEXT_PRICE] Formatted price_levels for API: {price_levels}")
        
        # Prepare enhanced result with rich model performance metrics
        result = {
            "symbol": symbol,
            "timeframe": timeframe,
            "predicted_price": predicted_price,
            "last_price": last_price,
            "prediction_time": next_timestamp.isoformat(),
            "current_time": datetime.now(pytz.UTC).isoformat(),
            "probability": probability.get('probability', 0.5),
            "confidence_factors": probability.get('confidence_factors', {}),
            "price_levels": price_levels,  # Add the formatted support/resistance levels
            "model_performance": {
                "r2_score": final_metrics.get('r2', final_metrics.get('val_r2', 0.0)),
                "mape": final_metrics.get('mape', final_metrics.get('val_mape', 0.0)),
                "mae": final_metrics.get('mae', final_metrics.get('val_mae', 0.0)),
                "rmse": final_metrics.get('rmse', final_metrics.get('val_rmse', 0.0)),
                "validation_loss": final_metrics.get('val_loss', 0.0),
                "trained_on_demand": final_metrics.get('trained_on_demand', force_retrain),
                "best_epoch": final_metrics.get('best_epoch', 0),
                "model_architecture": {
                    "lookback": lookback,
                    "input_size": len(feature_names),
                    "hidden_size": timeframe_config.get('hidden_size', 128),
                    "num_layers": timeframe_config.get('num_layers', 2),
                    "dropout": timeframe_config.get('dropout', 0.3)
                }
            }
        }
        
        logger.info(f"Prediction result for {symbol} {timeframe}: {result}")
        return result
        
    except Exception as e:
        logger.error(f"Error predicting price for {symbol} {timeframe}: {str(e)}")
        raise


async def prepare_features(symbol: str, timeframe: str) -> tuple:
    """Prepare features for model input with proper shapes"""
    try:
        # Get historical data
        data_fetcher = DataFetcher()
        df = await data_fetcher.get_merged_data(symbol, timeframe)
        
        if df is None or df.empty:
            logger.error(f"Failed to fetch data for {symbol} {timeframe}")
            raise ValueError(f"No data available for {symbol} {timeframe}")
        
        # Forward fill missing values, then backward fill any remaining
        df = df.ffill().bfill()
        
        # Generate technical indicators
        df_with_ta, _ = data_fetcher._add_technical_indicators(df.copy(), timeframe)
        
        if df_with_ta is None or df_with_ta.empty:
            logger.error(f"Failed to generate features for {symbol} {timeframe}")
            raise ValueError(f"Feature generation failed for {symbol} {timeframe}")
        
        # Ensure all required columns are present
        required_columns = ['Open', 'High', 'Low', 'Close', 'Volume']
        missing_columns = [col for col in required_columns if col not in df_with_ta.columns]
        if missing_columns:
            raise ValueError(f"Missing required columns: {missing_columns}")
        
        # Create target series based on timeframe
        # For shorter timeframes (1h), predict percentage change to avoid unrealistic volatility
        # For longer timeframes (>1h), predict absolute price
        if timeframe in ['1m', '5m', '15m', '30m', '1h']:
            # Predict percentage change for short timeframes
            target_series = df_with_ta['Close'].pct_change(periods=1).shift(-1) * 100  # Convert to percentage
            logger.info(f"Using percentage change target for {timeframe} timeframe")
        else:
            # Predict absolute price for longer timeframes
            target_series = df_with_ta['Close'].shift(-1)
            logger.info(f"Using absolute price target for {timeframe} timeframe")
        
        # Drop the last row since it won't have a target value
        df_with_ta = df_with_ta[:-1]
        target_series = target_series[:-1]
        
        # Handle any remaining NaN values
        if df_with_ta.isna().any().any():
            logger.warning("NaN values found in features, filling with forward fill then backward fill")
            df_with_ta = df_with_ta.ffill().bfill()
        
        if target_series.isna().any():
            logger.warning("NaN values found in target series, filling with forward fill then backward fill")
            target_series = target_series.ffill().bfill()
        
        # Log shapes for debugging
        logger.debug(f"Features shape: {df_with_ta.shape}")
        logger.debug(f"Target shape: {target_series.shape}")
        
        # CRITICAL FIX: Drop rows with NaNs introduced by feature engineering (e.g., momentum, lag)
        # Align features and target by index after dropping NaNs
        features_df = df_with_ta.dropna()
        target_series = target_series[features_df.index]

        if features_df.empty:
            raise ValueError("No data left after cleaning NaNs. Check feature generation.")
        
        return features_df, target_series
        
    except Exception as e:
        logger.error(f"Error preparing features: {str(e)}")
        raise

def check_data_drift(old_data, new_data, feature_names):
    """Detect data drift using KS test with multiple features"""
    if len(old_data) < 10 or len(new_data) < 10:
        return True
    # Only check first 8 features for efficiency, or all if less than 8
    features_to_check = feature_names[:min(8, len(feature_names))]
    for i, feature in enumerate(features_to_check):
        stat, p = ks_2samp(old_data[:, i], new_data[:, i])
        if p < 0.01:
            logger.info(f"Data drift detected in feature {feature}")
            return True
    return False

async def get_model(symbol, timeframe="24h", lookback=None, force_retrain=False):
    """Get or train model for given symbol and timeframe using robust ModelManager
    
    Args:
        force_retrain: If True, delete existing model and train fresh (default: False)
    """
    try:
        from controllers.model_manager import ModelManager
        
        # Get hyperparameters from TIMEFRAME_MAP
        timeframe_config = simple_config.TIMEFRAME_MAP.get(timeframe, {})
        if lookback is None:
            lookback = timeframe_config.get('lookback', 72)
        
        # Initialize ModelManager
        model_manager = ModelManager()
        
        # Get training data to determine input size and feature names
        X_train, y_train, X_val, y_val, feature_scaler, target_scaler, feature_names, features_df = await _get_and_prepare_training_data(
            symbol, timeframe, lookback
        )
        
        # Get model configuration
        input_size = X_train.shape[2]  # Number of features
        hidden_size = timeframe_config.get('hidden_size', 128)
        num_layers = timeframe_config.get('num_layers', 2)
        dropout = timeframe_config.get('dropout', 0.3)
        batch_size = timeframe_config.get('batch_size', 64)
        learning_rate = timeframe_config.get('learning_rate', 0.001)
        
        # Expected configuration for current model
        expected_config = {
            'input_size': input_size,
            'hidden_size': hidden_size,
            'num_layers': num_layers,
            'dropout': dropout,
            'feature_list': feature_names,
            'lookback': lookback
        }
        
        model = None
        # If force_retrain is True, delete existing model to ensure fresh training
        if force_retrain:
            logger.info(f"Force retrain requested for {symbol} {timeframe}, deleting existing model")
            model_manager.delete_model(symbol, timeframe)
        else:
            # Try to load existing model
            loaded_model, loaded_feature_scaler, loaded_target_scaler, config, _ = model_manager.load_model_and_scalers(symbol, timeframe)
            
            if loaded_model and model_manager.check_model_compatibility(symbol, timeframe, expected_config):
                logger.info(f"Using existing compatible model for {symbol} {timeframe}")
                training_metrics = model_manager.get_latest_metrics(symbol, timeframe) or {}
                loaded_model.eval() # Ensure model is in eval mode
                return loaded_model, loaded_feature_scaler, loaded_target_scaler, feature_names, training_metrics
            elif loaded_model:
                logger.warning(f"Model architecture mismatch for {symbol} {timeframe}, will retrain.")
                model_manager.delete_model(symbol, timeframe)

        # If we reach here, we need to train a new model
        logger.info(f"Training new model for {symbol} {timeframe}")
        
        # Create new model
        model = BiLSTMWithAttention(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout
        )
        
        # Train model
        model, training_history = train_model(
            X_train, y_train, X_val, y_val, model, timeframe, symbol,
            batch_size, learning_rate,
            feature_names=feature_names,
            feature_scaler=feature_scaler,
            target_scaler=target_scaler
        )
        
        # Extract metrics from training
        training_metrics = training_history.get('best_metrics', {})
        training_metrics['trained_on_demand'] = True
        
        # Save the newly trained model
        training_info = {
            'symbol': symbol,
            'timeframe': timeframe,
            'training_date': datetime.now(pytz.UTC).isoformat(),
            'data_points': len(X_train) + len(X_val),
            'training_history': training_history
        }
        
        success = model_manager.save_model_and_scalers(
            model, feature_scaler, target_scaler, 
            symbol, timeframe, expected_config, training_info, training_metrics
        )
        
        if success:
            logger.info(f"Successfully saved new model for {symbol} {timeframe} with metrics: {training_metrics}")
        else:
            logger.error(f"Failed to save new model for {symbol} {timeframe}")
            raise IOError(f"Could not save model for {symbol} {timeframe}")
            
        model.eval() # Ensure model is in eval mode before returning
        return model, feature_scaler, target_scaler, feature_names, training_metrics
        
    except Exception as e:
        logger.error(f"Error in get_model for {symbol} {timeframe}: {str(e)}")
        raise

# OLD APPROACH - Keeping for backward compatibility and troubleshooting
async def get_model_old_approach(symbol, timeframe="24h", lookback=None, force_retrain=False):
    """OLD approach for model training with explicit file paths - for debugging scale issues"""
    try:
        # Get data
        X_train, y_train, X_val, y_val, feature_scaler, target_scaler, feature_names, features_df = await _get_and_prepare_training_data(
            symbol, timeframe, lookback
        )
        
        # Get hyperparameters from TIMEFRAME_MAP
        timeframe_config = simple_config.TIMEFRAME_MAP.get(timeframe, {})
        if lookback is None:
            lookback = timeframe_config.get('lookback', 72)
        
        input_size = X_train.shape[2]
        hidden_size = timeframe_config.get('hidden_size', 128)
        num_layers = timeframe_config.get('num_layers', 2)
        dropout = timeframe_config.get('dropout', 0.3)
        batch_size = timeframe_config.get('batch_size', 32)
        learning_rate = timeframe_config.get('learning_rate', 1e-4)
        max_lr = timeframe_config.get('max_lr', 2e-5)
        
        # Log complete hyperparameter configuration
        logger.info(
            f"Initializing {timeframe} model with configuration:\n"
            f"Architecture: lookback={lookback}, input_size={input_size}, hidden_size={hidden_size}, "
            f"num_layers={num_layers}, dropout={dropout}\n"
            f"Training: batch_size={batch_size}, learning_rate={learning_rate}, max_lr={max_lr}, "
            f"loss=HuberLoss, optimizer=AdamW, scheduler=OneCycleLR"
        )
        
        # Define file paths
        model_dir = os.path.join(MODEL_PATH, f"{symbol}_{timeframe}")
        model_path = os.path.join(model_dir, "model.pth")
        scaler_feature_path = os.path.join(model_dir, "feature_scaler.joblib")
        scaler_target_path = os.path.join(model_dir, "target_scaler.joblib")
        
        # If force_retrain is True, delete existing model files to ensure fresh training
        if force_retrain and os.path.exists(model_dir):
            logger.info(f"Force retrain requested for {symbol} {timeframe}, deleting existing model")
            import shutil
            shutil.rmtree(model_dir)
            logger.info(f"Deleted existing model directory: {model_dir}")
        
        # Try to load existing model (only if force_retrain is False)
        if not force_retrain and os.path.exists(model_path) and os.path.exists(scaler_feature_path) and os.path.exists(scaler_target_path):
            logger.info(f"Loading existing model from {model_dir}")
            checkpoint = torch.load(model_path, map_location=torch.device('cpu'))
            
            # Extract model configuration with backward compatibility
            if 'config' in checkpoint:
                config = checkpoint['config']
            elif 'model_config' in checkpoint:
                config = checkpoint['model_config']
            else:
                logger.warning("No config found in checkpoint, using defaults")
                config = {}
            
            training_info = checkpoint.get('training_info', {})
            
            # Initialize model with loaded configuration
            model = BiLSTMWithAttention(
                input_size=config.get('input_size', input_size),
                hidden_size=config.get('hidden_size', hidden_size),
                num_layers=config.get('num_layers', num_layers),
                dropout=config.get('dropout', dropout)
            )
            
            # Load model state
            model.load_state_dict(checkpoint['model_state_dict'])
            model.eval()
            
            # Load scalers
            feature_scaler = joblib.load(scaler_feature_path)
            target_scaler = joblib.load(scaler_target_path)
            
            # Log loaded model information
            if training_info:
                logger.info(f"Loaded model trained at {training_info.get('training_time', 'unknown time')} "
                          f"with best metrics from epoch {training_info.get('best_epoch', 'unknown')}: "
                          f"Val Loss: {training_info.get('best_metrics', {}).get('val_loss', float('inf')):.4f}, "
                          f"MAPE: {training_info.get('best_metrics', {}).get('val_mape', float('inf')):.2f}%, "
                          f"R2: {training_info.get('best_metrics', {}).get('val_r2', float('-inf')):.4f}")
            
            # Extract training metrics
            training_metrics = training_info.get('best_metrics', {})
            training_metrics['trained_on_demand'] = False  # This was loaded from existing model
            
            return model, feature_scaler, target_scaler, feature_names, training_metrics
        
        # Create new model (force_retrain=True) or no existing model found
        if force_retrain:
            logger.info(f"Training fresh model for {symbol} {timeframe} (force retrain enabled)")
        else:
            logger.info(f"No existing model found for {symbol} {timeframe}, training new model")
            
        logger.info(f"Creating new model with input_size={input_size}, hidden_size={hidden_size}, "
                   f"num_layers={num_layers}, dropout={dropout}")
        model = BiLSTMWithAttention(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout
        )
        
        # Train model with optimized hyperparameters
        model, training_history = train_model(
            X_train, y_train,
            X_val, y_val,
            model, timeframe, symbol,
            batch_size, learning_rate,
            feature_names=feature_names,
            feature_scaler=feature_scaler,
            target_scaler=target_scaler
        )
        
        # Save scalers alongside the model
        os.makedirs(model_dir, exist_ok=True)
        joblib.dump(feature_scaler, scaler_feature_path)
        joblib.dump(target_scaler, scaler_target_path)
        logger.info(f"Saved feature and target scalers to {model_dir}")
        
        # Extract training metrics from the training history (if available)
        training_metrics = {}
        if training_history and 'best_metrics' in training_history:
            training_metrics = training_history['best_metrics']
        training_metrics['trained_on_demand'] = True  # This was freshly trained
        
        # Return all five required values consistently
        return model, feature_scaler, target_scaler, feature_names, training_metrics
        
    except Exception as e:
        logger.error(f"Error in get_model: {str(e)}", exc_info=True)
        raise

def validate_data_quality(data: pd.DataFrame) -> bool:
    """Validate data quality with comprehensive checks for training data"""
    try:
        if data.empty:
            logger.error("Empty dataset")
            return False
            
        # Check for minimum required data points based on timeframe
        timeframe_min_points = {
            "30m": 1000,  # ~2 weeks of data
            "1h": 720,    # ~1 month of data
            "4h": 360,    # ~2 months of data
            "24h": 365    # 1 year of data
        }
        
        timeframe = getattr(data, 'timeframe', '24h')  # Default to 24h if not specified
        min_required = timeframe_min_points.get(timeframe, 1000)
        
        if len(data) < min_required:
            logger.error(f"Insufficient data points for {timeframe}: {len(data)} < {min_required}")
            return False
            
        # Check for missing values
        missing_pct = data.isnull().sum() / len(data)
        for column, pct in missing_pct.items():
            if pct > 0.1:  # More than 10% missing in any column
                logger.error(f"Too many missing values in {column}: {pct:.2%}")
                return False
            
        # Check for price continuity and anomalies
        price_changes = data['Close'].pct_change().abs()
        max_allowed_change = {
            "30m": 0.1,   # 10% max change for 30m
            "1h": 0.15,   # 15% max change for 1h
            "4h": 0.2,    # 20% max change for 4h
            "24h": 0.3    # 30% max change for 24h
        }
        
        max_change = max_allowed_change.get(timeframe, 0.3)
        anomalies = price_changes[price_changes > max_change]
        if len(anomalies) > 0:
            logger.warning(f"Found {len(anomalies)} suspicious price changes > {max_change:.1%}")
            if len(anomalies) > len(data) * 0.05:  # If more than 5% of data points are anomalous
                logger.error("Too many anomalous price changes")
                return False
            
        # Check for zero or negative prices
        if (data['Close'] <= 0).any():
            logger.error("Found zero or negative prices")
            return False
            
        # Check for zero volume periods
        zero_volume_pct = (data['Volume'] == 0).mean()
        if zero_volume_pct > 0.1:  # More than 10% periods with zero volume
            logger.error(f"Too many zero volume periods: {zero_volume_pct:.2%}")
            return False
            
        # Ensure time series continuity
        time_diffs = data.index.to_series().diff().dropna()
        expected_diff = {
            "30m": pd.Timedelta(minutes=30),
            "1h": pd.Timedelta(hours=1),
            "4h": pd.Timedelta(hours=4),
            "24h": pd.Timedelta(days=1)
        }
        expected = expected_diff.get(timeframe)
        if expected:
            irregular_intervals = time_diffs != expected
            if irregular_intervals.sum() > len(data) * 0.05:  # More than 5% irregular intervals
                logger.error(f"Too many irregular time intervals for {timeframe}")
                return False
                
        # Check data freshness
        if isinstance(data.index, pd.DatetimeIndex):
            last_timestamp = data.index[-1]
            if pd.Timestamp.now(tz='UTC') - last_timestamp > pd.Timedelta(days=1):
                logger.warning("Data may be stale - last timestamp is more than 1 day old")
                
        # Check for required features
        required_features = ['Open', 'High', 'Low', 'Close', 'Volume']
        missing_features = [f for f in required_features if f not in data.columns]
        if missing_features:
            logger.error(f"Missing required features: {missing_features}")
            return False
            
        # Verify data types
        expected_types = {
            'Open': np.floating,
            'High': np.floating,
            'Low': np.floating,
            'Close': np.floating,
            'Volume': np.floating
        }
        for col, expected_type in expected_types.items():
            if not np.issubdtype(data[col].dtype, expected_type):
                logger.error(f"Incorrect data type for {col}: expected {expected_type}, got {data[col].dtype}")
                return False
                
        return True
            
    except Exception as e:
        logger.error(f"Data validation error: {str(e)}")
        return False

def mean_absolute_percentage_error(y_true, y_pred):
    """Calculate mean absolute percentage error"""
    y_true = np.array(y_true).ravel()
    y_pred = np.array(y_pred).ravel()
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

def get_data_stats(data_tensor: torch.Tensor, target_tensor: torch.Tensor) -> dict:
    """Calculate basic statistics for data and target tensors."""
    try:
        if data_tensor is None or target_tensor is None:
            return {
                'X_mean': float('nan'),
                'X_std': float('nan'),
                'y_mean': float('nan'),
                'y_std': float('nan')
            }
        
        # Ensure tensors are on CPU for numpy operations
        if torch.is_tensor(data_tensor):
            data_tensor = data_tensor.cpu()
        if torch.is_tensor(target_tensor):
            target_tensor = target_tensor.cpu()
            
        return {
            'X_mean': float(torch.mean(data_tensor).item()),
            'X_std': float(torch.std(data_tensor).item()),
            'y_mean': float(torch.mean(target_tensor).item()),
            'y_std': float(torch.std(target_tensor).item())
        }
    except Exception as e:
        logger.error(f"Error in get_data_stats: {str(e)}")
        return {
            'X_mean': float('nan'),
            'X_std': float('nan'),
            'y_mean': float('nan'),
            'y_std': float('nan')
        }

def train_model(X_train, y_train, X_val, y_val, model, timeframe, symbol, batch_size, learning_rate, feature_names=None, feature_scaler=None, target_scaler=None):
    """Train the model with proper tensor handling and advanced training techniques"""
    try:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)
        
        # Ensure input tensors have correct shape and are on the right device
        X_train = X_train.to(device)  # Shape: (batch, seq_len, features)
        y_train = y_train.to(device)  # Shape: (batch, 1)
        X_val = X_val.to(device)      # Shape: (batch, seq_len, features)
        y_val = y_val.to(device)      # Shape: (batch, 1)
        
        # Create data loaders 
        train_dataset = TensorDataset(X_train, y_train) 
        val_dataset = TensorDataset(X_val, y_val)
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size)
        
        # Initialize optimizer with weight decay
        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
        
        # Get max_lr from config for scheduler and logging
        current_max_lr = simple_config.TIMEFRAME_MAP[timeframe].get('max_lr', 5e-5)

        # Learning rate scheduler
        scheduler = torch.optim.lr_scheduler.OneCycleLR(
           
            optimizer,
            max_lr=current_max_lr,
            epochs=EPOCHS,
            steps_per_epoch=len(train_loader),
            pct_start=0.3,
            anneal_strategy='cos'
        )
        
        # Initialize early stopping with increased patience
        early_stopping = EarlyStopping(patience=simple_config.TIMEFRAME_MAP[timeframe].get('early_stopping_patience', 40), min_delta=1e-4)
        
        # Training history
        history = {
            'train_loss': [], 'val_loss': [], 'val_mape': [], 'val_r2': [], 'learning_rates': []
        }
        
        # Initialize best metrics tracking
        best_val_loss = float('inf')
        best_model_state = None
        best_epoch = None
        best_metrics = None
        
        logger.info(f"Starting training for {symbol} ({timeframe}) with: batch_size={batch_size}, initial_lr={learning_rate}, max_lr(OneCycle)={current_max_lr:.2e}, loss=HuberLoss, optimizer=AdamW")
        
        for epoch in range(EPOCHS):
            model.train()
            train_losses = []
            
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = model(batch_X)
               
                outputs_squeezed = outputs.squeeze(-1)
               
               
                batch_y_squeezed = batch_y.squeeze(-1)
                
                loss = F.huber_loss(outputs_squeezed, batch_y_squeezed, reduction='mean', delta=1.0)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                scheduler.step()
                train_losses.append(loss.item())
            
            model.eval()
            val_losses = []
            val_predictions_scaled = []
            val_actuals_scaled = []
            
            with torch.no_grad():
                for batch_X, batch_y in val_loader:
                    outputs = model(batch_X)
                    outputs_squeezed = outputs.squeeze(-1)
                    batch_y_squeezed = batch_y.squeeze(-1)
                    
                    val_loss = F.huber_loss(outputs_squeezed, batch_y_squeezed, reduction='mean', delta=1.0)
                    val_losses.append(val_loss.item())
                    val_predictions_scaled.extend(outputs_squeezed.cpu().numpy())
                    val_actuals_scaled.extend(batch_y_squeezed.cpu().numpy())
            
            avg_train_loss = np.mean(train_losses)
            avg_val_loss = np.mean(val_losses)
            
            # Calculate metrics on denormalized values
            if target_scaler:
                val_predictions_denorm = target_scaler.inverse_transform(np.array(val_predictions_scaled).reshape(-1, 1)).ravel()
                val_actuals_denorm = target_scaler.inverse_transform(np.array(val_actuals_scaled).reshape(-1, 1)).ravel()
            else:
                val_predictions_denorm = np.array(val_predictions_scaled)
                val_actuals_denorm = np.array(val_actuals_scaled)
            
            # Calculate MAPE differently for percentage vs absolute targets
            if timeframe in ['1m', '5m', '15m', '30m', '1h']:
                # For percentage change targets, MAPE is already in percentage points
                # Since both actual and predicted are percentage changes, the error is already meaningful
                val_mape = np.mean(np.abs(val_actuals_denorm - val_predictions_denorm))
                logger.info(f"[TRAINING] Calculated MAPE for percentage change target: {val_mape:.4f}%")
            else:
                # For absolute price targets, use traditional MAPE calculation
                safe_actuals = np.where(val_actuals_denorm == 0, 1e-8, val_actuals_denorm)
                val_mape = np.mean(np.abs((val_actuals_denorm - val_predictions_denorm) / safe_actuals)) * 100
                logger.info(f"[TRAINING] Calculated MAPE for absolute price target: {val_mape:.4f}%")
           
            val_r2 = r2_score(val_actuals_denorm, val_predictions_denorm)
            val_rmse = np.sqrt(mean_squared_error(val_actuals_denorm, val_predictions_denorm))
            
            # Track metrics
            current_metrics = {
                'val_loss': avg_val_loss,
                'val_mape': val_mape,
                'val_r2': val_r2,
                'val_rmse': val_rmse,
                'learning_rate': scheduler.get_last_lr()[0]
            }
            
            # Update history
            history['train_loss'].append(avg_train_loss)
            history['val_loss'].append(avg_val_loss)
            history['val_mape'].append(val_mape)
            history['val_r2'].append(val_r2)
            history['learning_rates'].append(scheduler.get_last_lr()[0])
            
            # Track absolute best model state based on validation loss
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                best_model_state = copy.deepcopy(model.state_dict())
                best_epoch = epoch
                best_metrics = current_metrics.copy()
                logger.info(f"New absolute best model at epoch {epoch}: "
                          f"Val Loss: {avg_val_loss:.4f}, "
                          f"MAPE: {val_mape:.2f}%, "
                          f"R2: {val_r2:.4f}")
                
                # Save the best model checkpoint immediately
                model_dir = os.path.join(MODEL_PATH, f"{symbol}_{timeframe}")
                os.makedirs(model_dir, exist_ok=True)
                checkpoint = {
                    'model_state_dict': best_model_state,
                    'config': {
                        'input_size': model.input_size,
                        'hidden_size': model.hidden_size,
                        'num_layers': model.num_layers,
                        'dropout': model.dropout_rate,
                        'feature_list': feature_names,
                        'lookback': X_train.size(1)
                    },
                    'training_info': {
                        'best_epoch': best_epoch,
                        'best_metrics': best_metrics,
                        'training_time': datetime.now().isoformat(),
                        'timeframe': timeframe,
                        'symbol': symbol
                    }
                }
                torch.save(checkpoint, os.path.join(model_dir, "model.pth"))
            
            # Check early stopping
            if early_stopping.step(avg_val_loss, epoch, current_metrics, copy.deepcopy(model.state_dict())):
                logger.info(f"Early stopping triggered at epoch {epoch}. "
                          f"Best model was from epoch {best_epoch} with "
                          f"Val Loss: {best_val_loss:.4f}, "
                          f"MAPE: {best_metrics['val_mape']:.2f}%, "
                          f"R2: {best_metrics['val_r2']:.4f}")
                break
            else:
                logger.info(f"Epoch {epoch}/{EPOCHS-1} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, "
                           f"Val MAPE: {val_mape:.2f}%, Val R2: {val_r2:.4f}, LR: {scheduler.get_last_lr()[0]:.2e}")
        
        # Always load the absolute best model state at the end of training
        if best_model_state:
            model.load_state_dict(best_model_state)
            logger.info(f"Loaded absolute best model state from epoch {best_epoch} with metrics: "
                      f"Val Loss: {best_metrics['val_loss']:.4f}, "
                      f"MAPE: {best_metrics['val_mape']:.2f}%, "
                      f"R2: {best_metrics['val_r2']:.4f}")
        
        # Add best metrics to history for consumption by get_model
        history['best_metrics'] = best_metrics
        history['best_epoch'] = best_epoch
        
        return model, history
        
    except Exception as e:
        logger.error(f"Error in train_model for {symbol} {timeframe}: {str(e)}", exc_info=True)
        raise

def monitor_gradients(model):
    """Monitor gradient statistics during training"""
    try:
        grad_stats = {}
        for name, param in model.named_parameters():
            if param.grad is not None:
                grad_stats[name] = {
                    'mean': float(param.grad.mean().item()),
                    'std': float(param.grad.std().item()),
                    'norm': float(param.grad.norm().item())
                }
                
                # Check for vanishing/exploding gradients
                if grad_stats[name]['norm'] < 1e-7:
                    logger.warning(f"Possible vanishing gradients in {name}")
                elif grad_stats[name]['norm'] > 1e3:
                    logger.warning(f"Possible exploding gradients in {name}")
        
        return grad_stats
    except Exception as e:
        logger.error(f"Error monitoring gradients: {str(e)}")
        return {}

def save_training_curves(train_losses, val_losses, val_mapes, val_rmses, val_r2s, model_dir):
    """Save training curves data to JSON file"""
    try:
        curves_data = {
            'train_losses': [float(x) for x in train_losses],
            'val_losses': [float(x) for x in val_losses],
            'val_mapes': [float(x) for x in val_mapes],
            'val_rmses': [float(x) for x in val_rmses],
            'val_r2s': [float(x) for x in val_r2s]
        }
        
        curves_file = os.path.join(model_dir, 'training_curves.json')
        with open(curves_file, 'w') as f:
            json.dump(curves_data, f, indent=4)
            
        logger.info(f"Saved training curves to {curves_file}")
    except Exception as e:
        logger.error(f"Error saving training curves: {str(e)}")
        # Don't raise the error since this is not critical for model operation

def prepare_data_for_prediction(data: pd.DataFrame, timeframe: str) -> tuple:
    """Prepare data for prediction using enhanced preprocessing pipeline"""
    try:
        logger.info("Starting data preparation pipeline")
        
        # Initialize preprocessor and augmentor
        preprocessor = DataPreprocessor(timeframe)
        augmentor = DataAugmentor(timeframe)
        
        # 1. Preprocess data
        logger.info("Running preprocessing pipeline")
        processed_data = preprocessor.process(data)
        
        # 2. Add engineered features
        logger.info("Running feature engineering and augmentation")
        augmented_data = augmentor.process(processed_data)
        
        # 3. Prepare features and target
        feature_cols = [col for col in augmented_data.columns if col != 'target']
        X = augmented_data[feature_cols].values
        y = augmented_data['Close'].values
        
        # 4. Create sequences for LSTM
        lookback = simple_config.TIMEFRAME_MAP[timeframe]["lookback"]
        X_sequences = []
        y_sequences = []
        
        for i in range(len(X) - lookback):
            X_sequences.append(X[i:i+lookback])
            y_sequences.append(y[i+lookback])
            
        X_sequences = np.array(X_sequences)
        y_sequences = np.array(y_sequences)
        
        # 5. Split data
        split_idx = int(len(X_sequences) * (1 - VALIDATION_SPLIT))
        X_train = X_sequences[:split_idx]
        X_val = X_sequences[split_idx:]
        y_train = y_sequences[:split_idx]
        y_val = y_sequences[split_idx:]
        
        # 6. Verify data quality
        if len(X_train) < 100 or len(X_val) < 20:
            raise ValueError(f"Insufficient data after sequence creation: train={len(X_train)}, val={len(X_val)}")
            
        # Log data shapes and basic statistics
        logger.info(f"Training set shape: {X_train.shape}")
        logger.info(f"Validation set shape: {X_val.shape}")
        logger.info(f"Number of features: {X_train.shape[2]}")
        logger.info(f"Feature names: {feature_cols}")
        
        # Calculate and log basic statistics
        train_stats = {
            'mean': np.mean(y_train),
            'std': np.std(y_train),
            'min': np.min(y_train),
            'max': np.max(y_train)
        }
        logger.info(f"Training set statistics: {train_stats}")
        
        return X_train, X_val, y_train, y_val, preprocessor.feature_scalers
        
    except Exception as e:
        logger.error(f"Error in data preparation: {str(e)}")
        raise

class DataPreprocessor:
    """Comprehensive data preprocessing pipeline"""
    def __init__(self, timeframe: str):
        self.timeframe = timeframe
        self.scaler = None
        self.feature_scalers = {}
        self.outlier_thresholds = {}
        
    def detect_outliers(self, data: pd.DataFrame, feature: str, n_std: float = 3) -> pd.Series:
        """Detect outliers using z-score method"""
        z_scores = np.abs((data[feature] - data[feature].mean()) / data[feature].std())
        return z_scores > n_std
        
    def handle_outliers(self, data: pd.DataFrame) -> pd.DataFrame:
        """Handle outliers in price and volume data"""
        try:
            df = data.copy()
            
            # Define features to check for outliers
            price_features = ['Open', 'High', 'Low', 'Close']
            volume_features = ['Volume']
            
            # Handle price outliers
            for feature in price_features:
                outliers = self.detect_outliers(df, feature)
                if outliers.any():
                    logger.warning(f"Found {outliers.sum()} outliers in {feature}")
                    # Use rolling median to replace outliers
                    window = 5 if self.timeframe in ["30m", "1h"] else 3
                    df.loc[outliers, feature] = df[feature].rolling(window=window, center=True).median()
            
            # Handle volume outliers
            for feature in volume_features:
                outliers = self.detect_outliers(df, feature, n_std=4)  # More permissive for volume
                if outliers.any():
                    logger.warning(f"Found {outliers.sum()} outliers in {feature}")
                    # Use rolling median for volume outliers
                    window = 5 if self.timeframe in ["30m", "1h"] else 3
                    df.loc[outliers, feature] = df[feature].rolling(window=window, center=True).median()
            
            return df
            
        except Exception as e:
            logger.error(f"Error handling outliers: {str(e)}")
            return data
            
    def normalize_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """Normalize features using appropriate scaling methods"""
        try:
            df = data.copy()
            
            # Price features - use robust scaling
            price_features = ['Open', 'High', 'Low', 'Close']
            if 'price_scaler' not in self.feature_scalers:
                self.feature_scalers['price_scaler'] = RobustScaler()
                price_data = df[price_features].values
                self.feature_scalers['price_scaler'].fit(price_data)
            df[price_features] = self.feature_scalers['price_scaler'].transform(df[price_features])
            
            # Volume features - use log transformation and robust scaling
            volume_features = ['Volume']
            df[volume_features] = np.log1p(df[volume_features])
            if 'volume_scaler' not in self.feature_scalers:
                self.feature_scalers['volume_scaler'] = RobustScaler()
                volume_data = df[volume_features].values
                self.feature_scalers['volume_scaler'].fit(volume_data)
            df[volume_features] = self.feature_scalers['volume_scaler'].transform(df[volume_features])
            
            # Technical indicators - use standard scaling
            tech_features = [f for f in df.columns if f not in price_features + volume_features]
            if tech_features:
                if 'tech_scaler' not in self.feature_scalers:
                    self.feature_scalers['tech_scaler'] = RobustScaler()
                    tech_data = df[tech_features].values
                    self.feature_scalers['tech_scaler'].fit(tech_data)
                df[tech_features] = self.feature_scalers['tech_scaler'].transform(df[tech_features])
            
            return df
            
        except Exception as e:
            logger.error(f"Error normalizing features: {str(e)}")
            return data
            
    def add_temporal_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """Add temporal features based on timeframe"""
        try:
            df = data.copy()
            
            # Ensure index is datetime
            if not isinstance(df.index, pd.DatetimeIndex):
                df.index = pd.to_datetime(df.index)
            
            # Add cyclical time features
            if self.timeframe in ["30m", "1h"]:
                # Hour of day - cyclical encoding
                df['hour_sin'] = np.sin(2 * np.pi * df.index.hour / 24)
                df['hour_cos'] = np.cos(2 * np.pi * df.index.hour / 24)
                
                # Day of week - cyclical encoding
                df['day_sin'] = np.sin(2 * np.pi * df.index.dayofweek / 7)
                df['day_cos'] = np.cos(2 * np.pi * df.index.dayofweek / 7)
            
            if self.timeframe in ["4h", "24h"]:
                # Day of month - cyclical encoding
                df['dom_sin'] = np.sin(2 * np.pi * df.index.day / 31)
                df['dom_cos'] = np.cos(2 * np.pi * df.index.day / 31)
                
                # Month - cyclical encoding
                df['month_sin'] = np.sin(2 * np.pi * df.index.month / 12)
                df['month_cos'] = np.cos(2 * np.pi * df.index.month / 12)
            
            # Add trend features
            df['trend'] = np.arange(len(df)) / len(df)
            
            return df
            
        except Exception as e:
            logger.error(f"Error adding temporal features: {str(e)}")
            return data
            
    def handle_missing_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """Handle missing data with sophisticated methods"""
        try:
            df = data.copy()
            
            # Check for missing values
            missing = df.isnull().sum()
            if missing.any():
                logger.warning(f"Found missing values: {missing[missing > 0]}")
                
                # For each column with missing values
                for column in df.columns[df.isnull().any()]:
                    missing_pct = df[column].isnull().mean()
                    
                    if missing_pct < 0.05:  # Less than 5% missing
                        # Use interpolation for price data
                        if column in ['Open', 'High', 'Low', 'Close']:
                            df[column] = df[column].interpolate(method='time')
                        # Use forward fill for volume
                        elif column == 'Volume':
                            df[column] = df[column].ffill()
                        # Use interpolation for technical indicators
                        else:
                            df[column] = df[column].interpolate(method='linear')
                    else:
                        logger.error(f"Too many missing values in {column}: {missing_pct:.2%}")
                        raise ValueError(f"Column {column} has too many missing values")
            
            return df
            
        except Exception as e:
            logger.error(f"Error handling missing data: {str(e)}")
            return data
            
    def process(self, data: pd.DataFrame) -> pd.DataFrame:
        """Run the complete preprocessing pipeline"""
        try:
            df = data.copy()
            
            # 1. Handle missing data
            df = self.handle_missing_data(df)
            
            # 2. Handle outliers
            df = self.handle_outliers(df)
            
            # 3. Add temporal features
            df = self.add_temporal_features(df)
            
            # 4. Calculate technical indicators
            df = calculate_technical_indicators(df)
            
            # 5. Normalize features
            df = self.normalize_features(df)
            
            # Final validation
            if not validate_data_quality(df):
                raise ValueError("Data failed final validation after preprocessing")
            
            return df
            
        except Exception as e:
            logger.error(f"Error in preprocessing pipeline: {str(e)}")
            raise

class DataAugmentor:
    """Advanced feature engineering and data augmentation"""
    def __init__(self, timeframe: str):
        self.timeframe = timeframe
        self.feature_importance = {}
        
    def add_price_patterns(self, data: pd.DataFrame) -> pd.DataFrame:
        """Add candlestick pattern features"""
        try:
            df = data.copy()
            
            # Basic candlestick features
            df['body_size'] = df['Close'] - df['Open']
            df['upper_shadow'] = df['High'] - df[['Open', 'Close']].max(axis=1)
            df['lower_shadow'] = df[['Open', 'Close']].min(axis=1) - df['Low']
            df['body_to_shadow_ratio'] = df['body_size'].abs() / (df['upper_shadow'] + df['lower_shadow'] + 1e-8)
            
            # Candlestick patterns
            df['doji'] = (abs(df['body_size']) <= 0.1 * (df['High'] - df['Low'])).astype(float)
            df['hammer'] = ((df['lower_shadow'] > 2 * abs(df['body_size'])) & 
                          (df['upper_shadow'] <= 0.2 * df['lower_shadow'])).astype(float)
            df['shooting_star'] = ((df['upper_shadow'] > 2 * abs(df['body_size'])) & 
                                 (df['lower_shadow'] <= 0.2 * df['upper_shadow'])).astype(float)
            
            return df
            
        except Exception as e:
            logger.error(f"Error adding price patterns: {str(e)}")
            return data
            
    def add_volatility_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """Add advanced volatility features"""
        try:
            df = data.copy()
            
            # Parkinson volatility
            df['parkinson_vol'] = np.sqrt(1 / (4 * np.log(2)) * 
                                        ((df['High'] / df['Low']).apply(np.log))**2)
            
            # Garman-Klass volatility
            df['garman_klass_vol'] = np.sqrt(0.5 * np.log(df['High'] / df['Low'])**2 - 
                                            (2 * np.log(2) - 1) * np.log(df['Close'] / df['Open'])**2)
            
            # Rolling volatility measures
            windows = [5, 10, 20] if self.timeframe in ["30m", "1h"] else [3, 5, 10]
            
            for window in windows:
                # Standard deviation of returns
                df[f'return_vol_{window}'] = df['Close'].pct_change().rolling(window).std()
                
                # Range-based volatility
                df[f'range_vol_{window}'] = ((df['High'] - df['Low']) / df['Close']).rolling(window).mean()
                
                # Realized volatility
                df[f'realized_vol_{window}'] = np.sqrt(
                    (df['Close'].pct_change()**2).rolling(window).sum() / window
                )
            
            return df
            
        except Exception as e:
            logger.error(f"Error adding volatility features: {str(e)}")
            return data
            
    def add_momentum_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """Add momentum and trend features"""
        try:
            df = data.copy()
            
            # Define lookback periods based on timeframe
            if self.timeframe in ["30m", "1h"]:
                periods = [12, 24, 48, 96]  # 6h, 12h, 24h, 48h for hourly data
            else:
                periods = [3, 5, 10, 20]
            
            for period in periods:
                # ROC (Rate of Change)
                df[f'roc_{period}'] = df['Close'].pct_change(period)
                
                # Momentum
                df[f'momentum_{period}'] = df['Close'] - df['Close'].shift(period)
                
                # Acceleration
                df[f'acceleration_{period}'] = df[f'momentum_{period}'] - df[f'momentum_{period}'].shift(1)
                
                # Trend strength
                df[f'trend_strength_{period}'] = df['Close'].rolling(period).mean() / df['Close'] - 1
            
            # Add RSI variations
            df['rsi_smooth'] = df['RSI'].rolling(3).mean()
            df['rsi_impulse'] = df['RSI'] - df['RSI'].shift(3)
            
            return df
            
        except Exception as e:
            logger.error(f"Error adding momentum features: {str(e)}")
            return data
            
    def add_volume_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """Add advanced volume analysis features"""
        try:
            df = data.copy()
            
            # Volume momentum
            df['volume_momentum'] = df['Volume'].pct_change()
            
            # Volume weighted average price variations
            windows = [5, 10, 20] if self.timeframe in ["30m", "1h"] else [3, 5, 10]
            
            for window in windows:
                # VWAP
                df[f'vwap_{window}'] = (df['Volume'] * (df['High'] + df['Low'] + df['Close']) / 3).rolling(window).sum() / df['Volume'].rolling(window).sum()
                
                # Volume force
                df[f'volume_force_{window}'] = df['volume_momentum'] * df['Close'].pct_change()
                
                # Price-volume trend
                df[f'pvt_{window}'] = (df['Close'].pct_change() * df['Volume']).rolling(window).sum()
            
            # Volume profile
            df['volume_price_spread'] = (df['High'] - df['Low']) * df['Volume']
            df['volume_price_correlation'] = df['Close'].rolling(20).corr(df['Volume'])
            
            return df
            
        except Exception as e:
            logger.error(f"Error adding volume features: {str(e)}")
            return data
            
    def generate_synthetic_samples(self, data: pd.DataFrame) -> pd.DataFrame:
        """Generate synthetic samples for rare events"""
        try:
            df = data.copy()
            
            # Identify rare events (e.g., large price movements)
            returns = df['Close'].pct_change()
            std_dev = returns.std()
            rare_events = abs(returns) > 2 * std_dev  # More than 2 standard deviations
            
            if rare_events.sum() < len(df) * 0.1:  # Less than 10% rare events
                logger.info("Generating synthetic samples for rare events")
                
                rare_samples = df[rare_events].copy()
                
                # Create variations of rare events
                for _ in range(3):  # Generate 3 variations for each rare event
                    variation = rare_samples.copy()
                    
                    # Add small random variations to features
                    for col in variation.select_dtypes(include=[np.number]).columns:
                        noise = np.random.normal(0, 0.01, len(variation))
                        variation[col] = variation[col] * (1 + noise)
                    
                    df = pd.concat([df, variation])
            
            return df.sort_index()
            
        except Exception as e:
            logger.error(f"Error generating synthetic samples: {str(e)}")
            return data
            
    def process(self, data: pd.DataFrame) -> pd.DataFrame:
        """Apply all feature engineering and augmentation steps"""
        try:
            df = data.copy()
            
            # 1. Add price patterns
            df = self.add_price_patterns(df)
            
            # 2. Add volatility features
            df = self.add_volatility_features(df)
            
            # 3. Add momentum features
            df = self.add_momentum_features(df)
            
            # 4. Add volume features
            df = self.add_volume_features(df)
            
            # 5. Generate synthetic samples if needed
            if len(df) > 100:  # Only for sufficient data
                df = self.generate_synthetic_samples(df)
            
            # Remove any features with too many missing values
            missing_pct = df.isnull().sum() / len(df)
            cols_to_drop = missing_pct[missing_pct > 0.1].index
            if not cols_to_drop.empty:
                logger.warning(f"Dropping columns with too many missing values: {cols_to_drop.tolist()}")
                df = df.drop(columns=cols_to_drop)
            
            # Fill remaining missing values
            df = df.ffill().bfill()
            
            return df
            
        except Exception as e:
            logger.error(f"Error in augmentation pipeline: {str(e)}")
            raise

def main(symbol: str, timeframe: str):
    """Command-line entry point for prediction"""
    try:
        logger.debug(f"Script started: Predicting for {symbol} with timeframe {timeframe}")
        print(f"Starting prediction for {symbol} with timeframe {timeframe}")
        if timeframe not in simple_config.TIMEFRAME_OPTIONS:
            error_msg = f"Invalid timeframe. Must be one of: {', '.join(simple_config.TIMEFRAME_OPTIONS)}"
            logger.error(error_msg)
            print(error_msg)
            sys.exit(1)
        if timeframe == "24h":
            result = ensemble_prediction(symbol)
        else:
            result = predict_next_price(symbol, timeframe)
        if "error" in result:
            logger.error(f"Prediction error: {result['error']}")
            print(f"Error: {result['error']}")
            sys.exit(1)
        logger.debug(f"Prediction result: {result}")
        print(f"Prediction result: {result}")
        return result
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}", exc_info=True)
        print(f"Unexpected error: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: python -m controllers.predictor <symbol> <timeframe>")
        sys.exit(1)
    main(sys.argv[1], sys.argv[2])

